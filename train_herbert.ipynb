{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 GeForce RTX 3090\n"
     ]
    }
   ],
   "source": [
    "for idx in range(torch.cuda.device_count()):\n",
    "    print(idx, torch.cuda.get_device_name(idx))\n",
    "device = torch.device('cuda:0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = 'models/politicalHerBERT'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['models/politicalBERT\\\\vocab.json', 'models/politicalBERT\\\\merges.txt']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "from tokenizers import ByteLevelBPETokenizer\n",
    "\n",
    "paths = [str(x) for x in Path(\".\").glob(\"**/*.txt\")]\n",
    "\n",
    "# Initialize a tokenizer\n",
    "tokenizer = ByteLevelBPETokenizer()\n",
    "\n",
    "# Customize training\n",
    "tokenizer.train(files=paths, vocab_size=52_000, min_frequency=2, special_tokens=[\n",
    "    \"<s>\",\n",
    "    \"<pad>\",\n",
    "    \"</s>\",\n",
    "    \"<unk>\",\n",
    "    \"<mask>\",\n",
    "])\n",
    "\n",
    "\n",
    "tokenizer.save_model(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<s>',\n",
       " 'Jestem',\n",
       " 'Ġkonserwa',\n",
       " 'tywny',\n",
       " ',',\n",
       " 'Ġale',\n",
       " 'ĠlubiÄĻ',\n",
       " 'ĠgejÃ³w',\n",
       " '.',\n",
       " '</s>']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tokenizers.implementations import ByteLevelBPETokenizer\n",
    "from tokenizers.processors import BertProcessing\n",
    "\n",
    "\n",
    "tokenizer = ByteLevelBPETokenizer(\n",
    "    model_path+\"/vocab.json\",\n",
    "    model_path+\"/merges.txt\",\n",
    ")\n",
    "\n",
    "tokenizer._tokenizer.post_processor = BertProcessing(\n",
    "    (\"</s>\", tokenizer.token_to_id(\"</s>\")),\n",
    "    (\"<s>\", tokenizer.token_to_id(\"<s>\")),\n",
    ")\n",
    "tokenizer.enable_truncation(max_length=512)\n",
    "\n",
    "tokenizer.encode(\"Jestem konserwatywny, ale lubię gejów.\").tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import XLMTokenizer, RobertaForMaskedLM, RobertaConfig\n",
    "\n",
    "config = RobertaConfig(\n",
    "    vocab_size=52_000,\n",
    "    max_position_embeddings=514,\n",
    "    num_attention_heads=12,\n",
    "    num_hidden_layers=6,\n",
    "    type_vocab_size=1,\n",
    ")\n",
    "\n",
    "#tokenizer = XLMTokenizer.from_pretrained(\"allegro/herbert-klej-cased-tokenizer-v1\")\n",
    "model = RobertaForMaskedLM(config=config)\n",
    "\n",
    "from transformers import RobertaTokenizerFast\n",
    "\n",
    "#tokenizer = RobertaTokenizerFast.from_pretrained(model_path, max_len=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForMaskedLM were not initialized from the model checkpoint at allegro/herbert-klej-cased-v1 and are newly initialized: ['lm_head.layer_norm.bias', 'lm_head.decoder.weight', 'lm_head.layer_norm.weight', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.decoder.bias', 'lm_head.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import XLMTokenizer, RobertaModel, RobertaForMaskedLM, RobertaTokenizerFast\n",
    "\n",
    "tokenizer=\"allegro/herbert-klej-cased-tokenizer-v1\"\n",
    "embed_model=\"allegro/herbert-klej-cased-v1\"\n",
    "\n",
    "tokenizer = RobertaTokenizerFast.from_pretrained(tokenizer)\n",
    "model = RobertaForMaskedLM.from_pretrained(embed_model, return_dict=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import LineByLineTextDataset\n",
    "\n",
    "dataset = LineByLineTextDataset(\n",
    "    tokenizer=tokenizer,\n",
    "    file_path=\"twitter_data/data/texts_only.txt\",\n",
    "    block_size=128,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForLanguageModeling\n",
    "\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer, mlm=True, mlm_probability=0.15\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer, TrainingArguments\n",
    "\n",
    "tr_args = TrainingArguments(\n",
    "    output_dir=model_path,\n",
    "    overwrite_output_dir=True,\n",
    "    num_train_epochs=30,\n",
    "    per_device_train_batch_size=32,\n",
    "    save_steps=10_000,\n",
    "    save_total_limit=10,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=tr_args,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=dataset,\n",
    "    #prediction_loss_only=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "        </style>\n",
       "      \n",
       "      <progress value='27516' max='55200' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [27516/55200 1:33:00 < 1:33:34, 4.93 it/s, Epoch 14.95/30]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>4.517534</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>3.324793</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>2.973333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>2.779016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>2.618653</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>2.539037</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3500</td>\n",
       "      <td>2.454619</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>2.345311</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4500</td>\n",
       "      <td>2.291287</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>2.259480</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5500</td>\n",
       "      <td>2.216096</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>2.159998</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6500</td>\n",
       "      <td>2.130256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7000</td>\n",
       "      <td>2.086492</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7500</td>\n",
       "      <td>2.074387</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8000</td>\n",
       "      <td>2.022934</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8500</td>\n",
       "      <td>2.007016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9000</td>\n",
       "      <td>1.989172</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9500</td>\n",
       "      <td>1.944539</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10000</td>\n",
       "      <td>1.934309</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10500</td>\n",
       "      <td>1.927105</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11000</td>\n",
       "      <td>1.895789</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11500</td>\n",
       "      <td>1.855148</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12000</td>\n",
       "      <td>1.847465</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12500</td>\n",
       "      <td>1.844129</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13000</td>\n",
       "      <td>1.820094</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13500</td>\n",
       "      <td>1.802781</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14000</td>\n",
       "      <td>1.788145</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14500</td>\n",
       "      <td>1.788102</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15000</td>\n",
       "      <td>1.777855</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15500</td>\n",
       "      <td>1.750750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16000</td>\n",
       "      <td>1.751109</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16500</td>\n",
       "      <td>1.729383</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17000</td>\n",
       "      <td>1.717227</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17500</td>\n",
       "      <td>1.711266</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18000</td>\n",
       "      <td>1.709703</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18500</td>\n",
       "      <td>1.681344</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19000</td>\n",
       "      <td>1.670070</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19500</td>\n",
       "      <td>1.653414</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20000</td>\n",
       "      <td>1.670836</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20500</td>\n",
       "      <td>1.664461</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21000</td>\n",
       "      <td>1.641563</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21500</td>\n",
       "      <td>1.638047</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22000</td>\n",
       "      <td>1.628625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22500</td>\n",
       "      <td>1.602648</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23000</td>\n",
       "      <td>1.591711</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23500</td>\n",
       "      <td>1.603758</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24000</td>\n",
       "      <td>1.595656</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24500</td>\n",
       "      <td>1.574531</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25000</td>\n",
       "      <td>1.577687</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25500</td>\n",
       "      <td>1.574336</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26000</td>\n",
       "      <td>1.556727</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26500</td>\n",
       "      <td>1.551383</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27000</td>\n",
       "      <td>1.551313</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27500</td>\n",
       "      <td>1.533508</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-9-3435b262f1ae>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtrainer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mc:\\users\\strza\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\transformers\\trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(self, model_path, trial)\u001b[0m\n\u001b[0;32m    754\u001b[0m                         \u001b[0mtr_loss\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtraining_step\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    755\u001b[0m                 \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 756\u001b[1;33m                     \u001b[0mtr_loss\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtraining_step\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    757\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_total_flos\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloating_point_ops\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    758\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\strza\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\transformers\\trainer.py\u001b[0m in \u001b[0;36mtraining_step\u001b[1;34m(self, model, inputs)\u001b[0m\n\u001b[0;32m   1068\u001b[0m                 \u001b[0mscaled_loss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1069\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1070\u001b[1;33m             \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1071\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1072\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\strza\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\torch\\tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[0;32m    219\u001b[0m                 \u001b[0mretain_graph\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    220\u001b[0m                 create_graph=create_graph)\n\u001b[1;32m--> 221\u001b[1;33m         \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    222\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    223\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\strza\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\torch\\autograd\\__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[0;32m    128\u001b[0m         \u001b[0mretain_graph\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    129\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 130\u001b[1;33m     Variable._execution_engine.run_backward(\n\u001b[0m\u001b[0;32m    131\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    132\u001b[0m         allow_unreachable=True)  # allow_unreachable flag\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.save_model(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at models/politicalHerBERT and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'sequence': '<s>Czy ja iesocjalistą? </s>',\n",
       "  'score': 0.1482255458831787,\n",
       "  'token': 1639,\n",
       "  'token_str': 'ie'},\n",
       " {'sequence': '<s>Czy ja stsocjalistą? </s>',\n",
       "  'score': 0.12938810884952545,\n",
       "  'token': 725,\n",
       "  'token_str': 'st'},\n",
       " {'sequence': '<s>Czy ja Äsocjalistą? </s>',\n",
       "  'score': 0.041076239198446274,\n",
       "  'token': 49385,\n",
       "  'token_str': 'Ä'},\n",
       " {'sequence': '<s>Czy ja kosocjalistą? </s>',\n",
       "  'score': 0.03023802489042282,\n",
       "  'token': 267,\n",
       "  'token_str': 'ko'},\n",
       " {'sequence': '<s>Czy ja osocjalistą? </s>',\n",
       "  'score': 0.028940103948116302,\n",
       "  'token': 178,\n",
       "  'token_str': 'o'}]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "fill_mask = pipeline(\n",
    "    \"fill-mask\",\n",
    "    model=model_path,\n",
    "    tokenizer=\"allegro/herbert-klej-cased-tokenizer-v1\"#model_path\n",
    ")\n",
    "fill_mask(\"Czy ja <mask> socjalistą?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at models/politicalBERT and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1.3988e-01, -4.8242e-01, -4.4797e-01,  1.2004e-01,  2.8562e-01,\n",
      "         -2.7454e-01, -1.0131e-01, -4.6635e-01, -5.8219e-01, -2.0793e-01,\n",
      "          4.1097e-01, -3.4779e-01,  1.0638e-01, -1.8089e-01, -5.7358e-01,\n",
      "          1.9686e-01, -3.3328e-01, -8.5908e-01,  1.2199e-01,  1.9801e-01,\n",
      "          5.2861e-01, -3.0298e-01, -5.6212e-01, -8.1941e-01,  1.6757e-01,\n",
      "          1.6900e-01, -8.4392e-01,  4.3225e-01,  6.0221e-01, -9.6487e-01,\n",
      "          4.7043e-01, -5.1503e-01, -2.2288e-01, -7.1512e-01, -3.3719e-01,\n",
      "         -1.3765e-01,  3.0455e-01,  3.0996e-01,  6.3490e-01, -4.4972e-01,\n",
      "         -2.0741e-02,  4.6612e-01, -8.7157e-01, -6.4219e-01, -2.1672e-01,\n",
      "          5.6638e-01,  4.9022e-01, -8.5509e-02, -8.9402e-01,  5.1555e-01,\n",
      "         -2.6077e-01, -4.7545e-01, -1.9073e-01,  4.5381e-01, -3.2618e-01,\n",
      "          2.5455e-01, -4.2948e-01,  1.8751e-01,  4.4860e-02, -6.3936e-01,\n",
      "          7.6363e-01, -3.2402e-01, -5.1184e-01, -2.9143e-01,  3.0719e-01,\n",
      "         -7.0315e-01,  1.3743e-01, -2.7380e-01, -3.4730e-01,  1.7482e-01,\n",
      "          4.5494e-01,  3.2940e-01, -2.2453e-01, -5.5730e-01, -1.7583e-01,\n",
      "          2.6431e-01,  3.6869e-01, -1.5866e-01,  2.1788e-01, -3.6565e-01,\n",
      "         -3.7005e-01,  9.9821e-02,  4.5972e-01, -6.2778e-02,  1.8671e-01,\n",
      "          4.0251e-01,  2.1854e-01,  5.6332e-02,  2.8022e-01, -2.4490e-01,\n",
      "          2.9877e-01, -5.5562e-01,  1.1584e-02,  3.4963e-01,  1.2932e-01,\n",
      "         -2.3830e-01,  6.4161e-01, -4.4096e-01,  7.3266e-01, -5.7786e-01,\n",
      "         -1.2909e-01, -5.6530e-01, -7.9530e-01, -6.3209e-01, -4.1636e-02,\n",
      "         -2.5715e-01,  7.5708e-02,  6.8936e-01, -3.3657e-01,  2.3831e-01,\n",
      "         -4.4460e-01, -7.9447e-02, -8.3200e-02,  2.9190e-01,  1.9502e-01,\n",
      "          9.1257e-02,  6.3689e-01, -3.6417e-01,  4.8283e-02, -4.8425e-01,\n",
      "          2.4888e-01,  3.4648e-01, -5.1585e-01, -2.6929e-02,  4.1185e-01,\n",
      "         -2.5215e-01,  3.6077e-01, -3.2969e-01, -4.3301e-01, -4.9867e-01,\n",
      "         -8.4743e-01,  6.9054e-01, -5.0408e-01, -7.0063e-01, -2.5393e-02,\n",
      "          2.4754e-01,  7.8468e-01,  4.2643e-01, -8.3508e-01,  2.1908e-01,\n",
      "         -5.1020e-01,  2.1271e-02, -4.4881e-01,  5.0512e-02, -4.5839e-01,\n",
      "         -3.1087e-01,  7.9938e-02,  1.3346e-01, -4.7182e-01, -3.1610e-01,\n",
      "         -5.4732e-01, -2.9634e-01,  1.6384e-01,  5.2549e-01, -7.6523e-01,\n",
      "         -4.7299e-01,  6.3414e-02,  4.8692e-01,  3.8774e-01,  3.0672e-01,\n",
      "         -8.8615e-01, -6.6017e-01, -5.0796e-01,  3.3290e-01,  1.7156e-01,\n",
      "         -4.4912e-01,  7.5905e-01, -1.8720e-01, -8.1992e-01,  2.5317e-01,\n",
      "          7.9836e-02, -2.6476e-01, -8.5478e-02, -4.1331e-01, -4.6294e-01,\n",
      "          6.9137e-01,  2.6190e-01,  3.8494e-01, -7.9549e-01, -2.1490e-01,\n",
      "         -4.0138e-01,  5.5544e-01, -3.4851e-01, -1.6957e-01, -4.0295e-02,\n",
      "         -5.7743e-01, -6.0053e-01,  1.5968e-01,  1.8910e-01,  7.8860e-01,\n",
      "          2.0851e-02,  2.0743e-02, -1.1034e-01,  5.4584e-02,  8.1008e-02,\n",
      "          3.3427e-01,  1.1502e-02, -2.5380e-01, -7.1070e-01,  8.1221e-01,\n",
      "          6.4569e-01, -4.9390e-01,  6.1473e-01,  3.0710e-03,  7.1083e-01,\n",
      "          2.5645e-01,  7.5150e-01,  5.8769e-01,  2.7588e-01,  3.5345e-01,\n",
      "          5.9370e-01,  3.3880e-01, -3.4056e-01,  1.5286e-02,  3.6964e-01,\n",
      "         -4.8680e-01, -7.9520e-01, -5.2943e-02, -4.0080e-01,  5.2691e-01,\n",
      "         -6.8570e-01, -8.4303e-01, -3.5036e-01,  7.6910e-01,  2.3605e-01,\n",
      "          1.3491e-01, -7.5841e-01,  6.5082e-01, -7.9403e-03, -7.1332e-01,\n",
      "          6.7630e-01,  5.3181e-01, -3.4071e-01,  2.2266e-01,  6.5443e-01,\n",
      "          3.3998e-01, -6.7644e-01, -5.6312e-02, -8.6074e-02, -3.0647e-01,\n",
      "         -1.9149e-01,  7.5738e-01, -3.8720e-01, -1.8168e-01, -4.9782e-02,\n",
      "          9.0526e-01, -2.9223e-01, -3.1603e-01, -6.1425e-01, -5.7067e-02,\n",
      "         -2.4946e-01, -7.2539e-01, -5.2630e-01, -1.4405e-01, -5.0867e-01,\n",
      "         -6.7831e-02,  7.7131e-01,  5.6381e-01, -1.0865e-01, -1.9228e-01,\n",
      "          5.5490e-02, -3.8439e-01, -4.9613e-01,  8.2920e-01,  6.2933e-01,\n",
      "          3.2168e-01,  1.7630e-01, -1.8964e-01, -1.6974e-01,  6.6507e-01,\n",
      "         -4.3437e-01, -3.0322e-01, -3.2459e-01, -1.6109e-01,  3.4765e-01,\n",
      "          1.5951e-01,  7.0698e-01, -5.4866e-01,  7.4505e-01, -3.1838e-01,\n",
      "         -4.7483e-01, -4.3048e-01, -7.5576e-01, -2.7648e-01,  4.3433e-01,\n",
      "         -1.5534e-01,  7.2012e-01, -1.2271e-02,  9.2695e-02, -3.3047e-01,\n",
      "          5.9873e-01, -8.6604e-01,  6.6917e-01, -3.3453e-02,  2.4059e-01,\n",
      "         -1.1236e-01, -1.3688e-01,  3.5563e-01,  5.2335e-02, -8.4339e-01,\n",
      "         -6.5319e-01,  2.5161e-01,  3.6497e-01,  3.6878e-01, -1.9974e-02,\n",
      "          7.0775e-01, -5.9379e-01,  5.8480e-02, -6.4849e-01, -6.1955e-01,\n",
      "         -4.5625e-02, -3.7595e-01, -4.1660e-01,  4.0971e-01,  3.2637e-02,\n",
      "         -5.4644e-01, -2.4490e-01, -3.8173e-01, -6.0752e-01,  6.7203e-01,\n",
      "          7.7419e-01, -6.6579e-01,  4.4615e-01, -1.7486e-01,  2.2371e-01,\n",
      "          8.5775e-02,  6.7865e-01,  6.6831e-01, -7.6567e-01, -7.0802e-01,\n",
      "         -3.9734e-01, -3.3990e-01, -3.5063e-01,  4.5585e-01,  4.7351e-01,\n",
      "         -3.9861e-01,  2.2671e-01, -7.7950e-01,  1.0220e-01, -4.9687e-01,\n",
      "         -5.2785e-01, -1.2403e-01,  1.9822e-01, -6.4632e-01,  3.2139e-01,\n",
      "          2.0495e-01,  6.9539e-01,  1.5628e-01, -5.0421e-01,  5.9593e-01,\n",
      "          4.6074e-01,  9.1441e-02, -3.0790e-01, -4.6038e-01,  2.0590e-01,\n",
      "          3.7617e-01,  5.8237e-01,  2.4406e-01, -4.2931e-01, -6.4743e-01,\n",
      "         -5.2147e-03, -6.5173e-01, -7.2065e-01, -1.4696e-01,  3.9269e-01,\n",
      "          4.8875e-01, -2.5781e-01,  9.0300e-01, -4.5957e-01, -8.2147e-01,\n",
      "         -7.6180e-02,  2.5588e-01, -5.1974e-01,  5.8499e-01,  8.8364e-01,\n",
      "          6.6621e-01, -9.5897e-02,  8.6930e-01, -5.1834e-01, -7.3893e-01,\n",
      "          1.1615e-01, -5.0130e-01, -4.9882e-01,  1.1875e-01,  8.0829e-02,\n",
      "         -6.2363e-01,  1.5501e-01, -5.9089e-01, -1.4623e-01, -2.5798e-01,\n",
      "         -9.0273e-01, -2.9221e-01,  3.9791e-01, -8.3586e-01,  2.7077e-01,\n",
      "         -3.5043e-02,  5.3735e-01,  7.0446e-02,  5.2870e-01,  7.4014e-01,\n",
      "          4.8992e-01, -2.3652e-01, -8.5953e-01, -5.9396e-01, -3.9076e-01,\n",
      "          9.8832e-02,  5.4958e-01, -5.8580e-01,  1.2127e-01, -8.4425e-02,\n",
      "         -7.1383e-02, -5.4949e-01, -7.5898e-01, -8.5626e-01, -3.2243e-01,\n",
      "         -6.2891e-01, -3.8201e-01,  4.7300e-01, -8.2664e-02,  7.3019e-02,\n",
      "         -7.6382e-01,  5.0490e-02,  6.7180e-01,  8.3557e-01,  2.4697e-02,\n",
      "          1.3245e-01,  5.9483e-01, -1.8231e-01, -1.9410e-01,  3.0405e-01,\n",
      "          3.6702e-02, -3.6206e-01,  1.2147e-01,  2.7824e-01, -6.9706e-01,\n",
      "          5.8681e-01, -6.3788e-01,  1.7256e-01,  3.6113e-01,  4.3410e-01,\n",
      "          3.2375e-01,  1.5473e-01, -1.0190e-01, -1.7277e-01, -5.8273e-01,\n",
      "         -2.4869e-01, -5.1116e-01,  2.1312e-01, -8.0444e-01, -2.7012e-01,\n",
      "          2.5683e-01, -2.4012e-01,  8.9379e-03, -1.4280e-01, -2.9238e-01,\n",
      "          3.3286e-01,  1.6518e-01,  3.7213e-01, -1.0987e-01, -8.8033e-01,\n",
      "          4.7725e-01, -8.4108e-01,  3.8595e-02,  6.8738e-01,  5.3730e-01,\n",
      "          2.2421e-01, -1.4585e-01, -4.5798e-01, -6.7400e-01, -3.5147e-01,\n",
      "         -5.6462e-02,  9.1909e-01,  1.8891e-01,  4.7840e-01, -3.9854e-01,\n",
      "          7.6983e-01, -2.6811e-01,  2.0760e-01, -1.5335e-02, -2.3944e-02,\n",
      "          1.9680e-01,  3.4997e-01, -8.5731e-03,  1.8907e-01, -7.6754e-02,\n",
      "         -7.3897e-01, -8.3570e-01,  7.7663e-02,  4.7119e-01, -1.1063e-01,\n",
      "          1.5676e-01, -6.3200e-01, -8.2275e-02, -3.5684e-01,  4.4592e-01,\n",
      "          2.3212e-02,  4.8357e-01,  1.7144e-01,  2.9327e-01, -6.2365e-01,\n",
      "         -5.1565e-01,  2.3303e-01,  9.5760e-02,  7.2470e-01, -1.8815e-01,\n",
      "          2.6199e-01,  1.4772e-01,  7.3105e-01, -1.1124e-01, -2.1592e-01,\n",
      "         -1.5723e-02, -3.1948e-01, -7.4880e-01,  3.2505e-01,  8.1527e-01,\n",
      "         -8.2915e-02, -3.8596e-01, -5.8534e-01,  8.0829e-01, -6.1143e-01,\n",
      "         -1.0560e-02, -1.0070e-01, -3.3788e-01,  1.8645e-01,  3.7938e-01,\n",
      "         -2.5064e-02,  2.4352e-01, -3.2802e-01, -8.1772e-01, -6.6860e-01,\n",
      "         -5.1872e-02, -9.7241e-02, -2.1635e-01, -6.6492e-01, -3.9253e-02,\n",
      "          1.6199e-01, -3.9970e-01, -8.3983e-01, -2.4787e-01,  5.1999e-01,\n",
      "         -1.0896e-01,  4.2185e-01,  5.6771e-01, -4.6610e-01, -4.6912e-01,\n",
      "         -3.2583e-01, -5.8528e-01,  4.8341e-03,  2.9502e-01,  1.2863e-01,\n",
      "          1.2789e-01,  6.3934e-01,  5.0980e-01,  2.9813e-01,  4.5683e-01,\n",
      "          7.3422e-01, -1.3791e-02,  3.6816e-01, -6.6305e-02,  3.3820e-01,\n",
      "          2.5648e-01,  2.5368e-01, -5.0520e-01,  2.4898e-01, -1.5942e-02,\n",
      "          3.6628e-01,  6.2513e-01, -5.9638e-02, -8.0411e-02, -8.5268e-03,\n",
      "         -4.1808e-01,  1.3342e-01, -4.5903e-01,  3.8524e-01,  2.1170e-01,\n",
      "         -6.7239e-01, -3.6051e-01,  1.2527e-01,  2.5953e-01, -3.3989e-01,\n",
      "         -4.5521e-01,  3.3032e-03, -3.3123e-01,  5.4572e-01, -6.5580e-01,\n",
      "         -9.0891e-01, -5.8937e-01,  4.5431e-01,  5.3246e-01,  1.1432e-01,\n",
      "         -2.5174e-01,  5.6544e-01,  4.2430e-01, -2.2966e-01, -1.1568e-01,\n",
      "          5.2870e-01,  1.6590e-01,  1.6433e-01, -4.3540e-01, -8.4315e-02,\n",
      "         -1.7079e-01,  5.0770e-01, -2.0719e-01, -9.5749e-02,  9.1529e-01,\n",
      "          7.4829e-01,  2.6546e-01, -3.6459e-01,  4.6085e-01,  5.8534e-02,\n",
      "         -4.5065e-01,  1.3829e-01,  5.3613e-01,  4.3581e-01,  1.8268e-01,\n",
      "          1.3325e-01, -4.3344e-01, -1.3995e-01, -6.1167e-01, -6.8110e-02,\n",
      "          2.6471e-01, -2.8878e-01, -5.8086e-01,  3.6438e-01,  4.1579e-01,\n",
      "         -6.5582e-01, -3.9464e-01,  4.2026e-01, -1.8432e-02,  2.8735e-01,\n",
      "         -8.9625e-02,  6.7029e-02, -3.5727e-01,  6.5904e-01, -1.0611e-01,\n",
      "          3.2209e-01, -2.6557e-01,  2.5222e-01, -4.8389e-01,  1.7444e-01,\n",
      "          1.3687e-01, -1.2604e-01,  1.2377e-01,  2.2994e-01, -3.9517e-01,\n",
      "         -6.9174e-01, -3.5152e-01,  5.1469e-01,  6.6080e-01,  7.1600e-01,\n",
      "          6.0791e-01, -8.2221e-01,  6.1737e-01,  3.2408e-01, -8.7873e-01,\n",
      "         -7.0408e-01,  8.3261e-01,  7.6160e-01, -6.4162e-01,  4.0431e-01,\n",
      "          2.6174e-01, -8.5077e-01, -3.4850e-01,  6.9481e-02,  8.2722e-01,\n",
      "         -2.5779e-01, -6.8933e-01,  1.8918e-01,  4.0382e-01,  1.8577e-01,\n",
      "          4.5881e-01, -4.9957e-01,  5.6990e-02,  2.2441e-02,  1.4778e-01,\n",
      "         -8.3399e-01,  6.7334e-01,  4.0714e-01,  3.5025e-01, -3.1618e-01,\n",
      "         -3.6547e-01, -4.8261e-01, -7.7541e-01, -1.2929e-01,  4.1038e-01,\n",
      "          2.3820e-01,  1.1432e-01,  2.1357e-01, -6.2324e-01, -8.1967e-01,\n",
      "          5.7097e-01,  2.4986e-01,  1.0920e-01, -4.7554e-01, -5.5675e-01,\n",
      "          2.5848e-01,  2.3701e-01,  4.3578e-01, -2.7702e-01, -6.9340e-01,\n",
      "          6.2682e-01, -6.0378e-01, -1.8721e-01,  8.3072e-01, -9.2646e-02,\n",
      "         -2.8035e-01,  6.8046e-02,  5.8431e-01,  6.8472e-01,  4.4705e-01,\n",
      "          5.7313e-01,  3.7489e-01,  2.6022e-01, -1.6853e-01, -2.0399e-01,\n",
      "         -3.9032e-01, -6.4502e-01,  7.4599e-01, -1.9482e-01, -6.4273e-01,\n",
      "          1.2224e-01, -4.0453e-01,  2.9842e-01, -1.2384e-01, -6.2383e-02,\n",
      "          1.6394e-01, -4.4204e-01, -2.5266e-02, -1.2280e-01,  3.1588e-01,\n",
      "         -7.9610e-01, -4.4900e-02,  4.9889e-01,  5.8568e-01, -2.4179e-04,\n",
      "          1.2840e-01,  1.6434e-01,  5.9551e-02, -2.8157e-01,  1.7878e-01,\n",
      "          2.7519e-01,  1.9356e-01,  3.3026e-01, -1.2776e-01,  2.4233e-01,\n",
      "          1.8328e-01, -2.1696e-01, -6.8815e-01, -4.5725e-01, -1.2714e-02,\n",
      "         -2.9490e-01,  3.8712e-01, -1.5768e-02, -2.1460e-01, -5.7555e-01,\n",
      "         -3.9648e-01,  5.4485e-01, -4.8058e-02, -6.3188e-01,  3.8022e-01,\n",
      "          5.7540e-01,  3.4381e-01, -5.4260e-01, -1.1768e-01, -2.1254e-01,\n",
      "          3.3234e-01,  7.6874e-02, -5.3549e-01]], grad_fn=<TanhBackward>)\n"
     ]
    }
   ],
   "source": [
    "from transformers import RobertaTokenizerFast, RobertaModel\n",
    "\n",
    "tokenizer = RobertaTokenizerFast.from_pretrained(\"models/politicalBERT\")\n",
    "embed_model = RobertaModel.from_pretrained(\"models/politicalBERT\", return_dict=True)\n",
    "\n",
    "text = 'Nie lubię konserwatystów'\n",
    "\n",
    "encoded = tokenizer(text, return_tensors='pt', padding=True)\n",
    "#encoded = {k: v.to(next(self.parameters()).device) for k, v in encoded.items()}\n",
    "embeddings = embed_model(**encoded)['pooler_output'].float()\n",
    "print(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
